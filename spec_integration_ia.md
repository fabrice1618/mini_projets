# SpÃ©cifications : IntÃ©gration IA pour la gestion de projets et tÃ¢ches

## Contexte

Permettre aux utilisateurs de gÃ©rer leurs projets et tÃ¢ches via des commandes en langage naturel. L'IA interprÃ¨te les demandes et exÃ©cute les modifications dans la base de donnÃ©es via un systÃ¨me de function calling.

## Solution recommandÃ©e pour Ã©tudiants (gratuite)

### Approche : Function Calling avec Ollama

Cette solution utilise le **function calling** (appel de fonctions), une technique oÃ¹ l'IA peut dÃ©cider d'appeler des fonctions Python pour effectuer des actions concrÃ¨tes (comme modifier la base de donnÃ©es).

#### SchÃ©ma de l'architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application NiceGUI (kanban_view_nice.py)        â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Interface utilisateur (Panneau prompt IA)                    â”‚  â”‚
â”‚  â”‚  - L'utilisateur saisit : "CrÃ©e un projet 'Stage entreprise'" â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                                         â”‚
â”‚                           â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ai_assistant.py                                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ 1. process_user_request(prompt)                         â”‚  â”‚  â”‚
â”‚  â”‚  â”‚    - ReÃ§oit le prompt de l'utilisateur                  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚    - Envoie Ã  Ollama avec liste des tools disponibles   â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                           â”‚                                   â”‚  â”‚
â”‚  â”‚                           â–¼                                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Ollama (LLM local - Llama 3.2)                          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ - Comprend : "je dois crÃ©er un projet"                  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ - DÃ©cide d'appeler : creer_projet("Stage entreprise")   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ - Retourne : {"tool": "creer_projet",                   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚               "args": {"titre": "Stage entreprise"}}    â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                           â”‚                                   â”‚  â”‚
â”‚  â”‚                           â–¼                                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ 2. execute_tool_call(tool_name, arguments)              â”‚  â”‚  â”‚
â”‚  â”‚  â”‚    - ReÃ§oit la dÃ©cision de l'IA                         â”‚  â”‚  â”‚
â”‚  â”‚  â”‚    - Appelle la fonction Python correspondante          â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                             â”‚                                       â”‚
â”‚                             â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Fonctions de base de donnÃ©es (projet_db.py, tache_db.py)    â”‚  â”‚
â”‚  â”‚  - creer_projet(conn, cursor, titre, ...)                    â”‚  â”‚
â”‚  â”‚  - modifier_projet(conn, cursor, projet_id, ...)             â”‚  â”‚
â”‚  â”‚  - creer_tache(conn, cursor, projet_id, ...)                 â”‚  â”‚
â”‚  â”‚  - lister_projets(conn, cursor, statut)                      â”‚  â”‚
â”‚  â”‚  - etc.                                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                                         â”‚
â”‚                           â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Base de donnÃ©es SQLite (database.db)                         â”‚  â”‚
â”‚  â”‚  - Table: projets                                             â”‚  â”‚
â”‚  â”‚  - Table: taches                                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Flux de fonctionnement dÃ©taillÃ©

1. **L'utilisateur Ã©crit** : "CrÃ©e un projet 'Stage entreprise' avec Ã©chÃ©ance au 15 juin"

2. **ai_assistant.py reÃ§oit le prompt** et envoie Ã  Ollama :
   ```python
   # Ollama reÃ§oit :
   # - Le prompt utilisateur
   # - La liste des tools disponibles avec leurs descriptions :
   {
     "name": "creer_projet",
     "description": "CrÃ©e un nouveau projet",
     "parameters": {
       "titre": "string (requis)",
       "description": "string (optionnel)",
       "statut": "string (dÃ©faut: 'ouvert')"
     }
   }
   ```

3. **Ollama analyse et dÃ©cide** :
   - Comprend qu'il faut crÃ©er un projet
   - Extrait les informations : titre="Stage entreprise", Ã©chÃ©ance="2025-06-15"
   - Retourne un appel de fonction structurÃ©

4. **ai_assistant.py exÃ©cute la fonction** :
   ```python
   conn, cursor = ouvrir_database()
   projet_id = creer_projet(conn, cursor,
                           titre="Stage entreprise",
                           description="Ã‰chÃ©ance: 15 juin")
   ```

5. **RÃ©sultat affichÃ©** : "âœ… Projet 'Stage entreprise' crÃ©Ã© avec succÃ¨s (ID: 5)"

#### Notre approche (Function Calling) :
- âœ… Tout intÃ©grÃ© dans une seule application Python
- âœ… Pas de serveur sÃ©parÃ© Ã  gÃ©rer
- âœ… Code simple et lisible pour l'apprentissage
- âœ… MÃªme rÃ©sultat fonctionnel pour l'utilisateur

### Technologie : Ollama avec Llama 3.2

**Ollama** est un outil qui permet d'exÃ©cuter des modÃ¨les d'IA localement sur votre ordinateur.

**Avantages pour les Ã©tudiants** :
- âœ… **100% gratuit** : Aucun coÃ»t API, aucun abonnement
- âœ… **Installation simple** : Une seule commande `curl -fsSL https://ollama.com/install.sh | sh`
- âœ… **Fonctionne hors ligne** : Pas besoin de connexion internet aprÃ¨s installation
- âœ… **Performant** : Llama 3.2 excellent pour les tÃ¢ches structurÃ©es comme le function calling
- âœ… **PÃ©dagogique** : Comprendre comment fonctionne une IA localement
- âœ… **Pas de donnÃ©es envoyÃ©es sur internet** : ConfidentialitÃ© garantie

**Llama 3.2** est le modÃ¨le recommandÃ© car :
- OptimisÃ© pour le function calling
- Taille raisonnable (quelques GB)
- Bonnes performances mÃªme sur ordinateurs moyens
- Gratuit et open-source

## Configuration nÃ©cessaire

### Configuration matÃ©rielle minimale

Pour faire fonctionner Ollama avec Llama 3.2, voici les besoins en ressources :

#### Espace disque
- **Ollama** : ~500 MB
- **Llama 3.2 (3B)** : ~2 GB (modÃ¨le lÃ©ger, recommandÃ© pour dÃ©buter)
- **Llama 3.2 (1B)** : ~1 GB (encore plus lÃ©ger, pour machines limitÃ©es)
- **Total recommandÃ©** : Au moins **5 GB d'espace libre** (pour l'installation et la marge de manÅ“uvre)

#### MÃ©moire RAM
- **Minimum** : 8 GB de RAM
- **RecommandÃ©** : 16 GB de RAM pour de meilleures performances
- Le modÃ¨le Llama 3.2 (3B) utilise environ 3-4 GB de RAM pendant l'exÃ©cution

#### Processeur
- **Compatible avec** : x86_64 (Intel/AMD), ARM64 (Apple Silicon, ARM Linux)
- **RecommandÃ©** : Processeur multi-cÅ“urs moderne (4 cÅ“urs ou plus)
- **Note** : GPU optionnel mais amÃ©liore les performances (NVIDIA CUDA, AMD ROCm, Apple Metal)

### Installation par systÃ¨me d'exploitation

#### ğŸ§ Linux

**PrÃ©requis** :
- Distribution moderne (Ubuntu 20.04+, Debian 11+, Fedora 37+, etc.)
- `curl` installÃ©

**Installation** :
```bash
# Installation d'Ollama en une commande
curl -fsSL https://ollama.com/install.sh | sh

# VÃ©rifier l'installation
ollama --version

# TÃ©lÃ©charger le modÃ¨le Llama 3.2 (3B)
ollama pull llama3.2

# Ou pour la version plus lÃ©gÃ¨re (1B)
ollama pull llama3.2:1b

# Tester le modÃ¨le
ollama run llama3.2
```

**DÃ©marrage automatique** :
```bash
# Ollama dÃ©marre automatiquement comme service systemd
# Pour vÃ©rifier le statut :
sudo systemctl status ollama

# Pour redÃ©marrer le service :
sudo systemctl restart ollama
```

**Emplacement des modÃ¨les** :
- ModÃ¨les stockÃ©s dans : `~/.ollama/models/`

---

#### ğŸ macOS

**PrÃ©requis** :
- macOS 11 Big Sur ou plus rÃ©cent
- Architecture Intel ou Apple Silicon (M1/M2/M3)

**Installation** :
```bash
# MÃ©thode 1 : Via curl (similaire Ã  Linux)
curl -fsSL https://ollama.com/install.sh | sh

# MÃ©thode 2 : TÃ©lÃ©charger l'application depuis le site
# Rendez-vous sur https://ollama.com/download
# TÃ©lÃ©chargez Ollama.app et glissez-le dans Applications

# VÃ©rifier l'installation
ollama --version

# TÃ©lÃ©charger Llama 3.2
ollama pull llama3.2

# Tester
ollama run llama3.2
```

**Notes pour Apple Silicon (M1/M2/M3)** :
- Excellentes performances grÃ¢ce Ã  l'accÃ©lÃ©ration Metal
- Llama 3.2 fonctionne trÃ¨s bien mÃªme sur Mac Mini M1 avec 8 GB RAM

**Emplacement des modÃ¨les** :
- ModÃ¨les stockÃ©s dans : `~/.ollama/models/`

**Interface graphique** :
- Ollama tourne en arriÃ¨re-plan (icÃ´ne dans la barre de menu)
- Clic sur l'icÃ´ne pour voir le statut et les modÃ¨les installÃ©s

---

#### ğŸªŸ Windows

**PrÃ©requis** :
- Windows 10/11 (64-bit)
- WSL2 recommandÃ© mais pas obligatoire (version native disponible)

**Installation (version native Windows)** :
```powershell
# TÃ©lÃ©charger l'installeur depuis https://ollama.com/download
# ExÃ©cuter OllamaSetup.exe

# Ou via PowerShell (si disponible) :
# TÃ©lÃ©charger et installer automatiquement
winget install Ollama.Ollama

# VÃ©rifier l'installation
ollama --version

# TÃ©lÃ©charger Llama 3.2
ollama pull llama3.2

# Tester
ollama run llama3.2
```

**Installation via WSL2 (mÃ©thode alternative)** :
```bash
# Dans WSL2 Ubuntu/Debian
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2
```

**Emplacement des modÃ¨les (Windows natif)** :
- ModÃ¨les stockÃ©s dans : `C:\Users\VotreNom\.ollama\models\`

**Notes Windows** :
- Service Ollama dÃ©marre automatiquement au dÃ©marrage de Windows
- Accessible via `http://localhost:11434`

---

### Installation du client Python

AprÃ¨s avoir installÃ© Ollama, installer le client Python :

```bash
# Activer votre environnement virtuel (recommandÃ©)
python -m venv venv

# Linux/macOS
source venv/bin/activate

# Windows
venv\Scripts\activate

# Installer le client Ollama Python
pip install ollama

# VÃ©rifier l'installation
python -c "import ollama; print('âœ“ Client Ollama Python installÃ© avec succÃ¨s')"

# Ou vÃ©rifier la version avec pip
pip show ollama | grep Version
```

### VÃ©rification de l'installation complÃ¨te

Test rapide pour vÃ©rifier que tout fonctionne :

```bash
# 1. VÃ©rifier qu'Ollama tourne
curl http://localhost:11434

# Devrait retourner : "Ollama is running"

# 2. Lister les modÃ¨les installÃ©s
ollama list

# Devrait afficher llama3.2

# 3. Test Python
python << EOF
import ollama
response = ollama.chat(model='llama3.2', messages=[
    {'role': 'user', 'content': 'Dis bonjour en franÃ§ais'}
])
print(response['message']['content'])
EOF
```

### RÃ©solution de problÃ¨mes courants

#### ProblÃ¨me : "Ollama is not running"
**Solution** :
```bash
# Linux
sudo systemctl start ollama

# macOS
# Lancer Ollama.app depuis Applications

# Windows
# Lancer "Ollama" depuis le menu DÃ©marrer
```

#### ProblÃ¨me : Pas assez de RAM
**Solution** :
- Utiliser le modÃ¨le plus lÃ©ger : `ollama pull llama3.2:1b`
- Fermer les applications gourmandes en mÃ©moire
- VÃ©rifier les processus avec `htop` (Linux/macOS) ou Gestionnaire des tÃ¢ches (Windows)

#### ProblÃ¨me : TÃ©lÃ©chargement lent
**Solution** :
- Le premier tÃ©lÃ©chargement peut prendre 10-30 minutes selon votre connexion
- Llama 3.2 (3B) fait ~2 GB
- Laisser le tÃ©lÃ©chargement se terminer complÃ¨tement

### Alternatives pour machines trÃ¨s limitÃ©es

Si votre ordinateur ne rÃ©pond pas aux exigences :

1. **Utiliser un modÃ¨le plus petit** :
   ```bash
   ollama pull llama3.2:1b  # Seulement 1 GB
   ```

2. **Services cloud gratuits** (pour tests uniquement) :
   - Google Colab (GPU gratuit limitÃ©)
   - Hugging Face Spaces
   - Note : Ces solutions nÃ©cessitent une connexion internet

## CapacitÃ©s de l'IA

L'IA pourra effectuer les opÃ©rations suivantes :

### 1. CRUD Projets
- CrÃ©er un nouveau projet
- Lire/consulter les informations d'un projet
- Modifier un projet existant (titre, description, statut)
- Supprimer un projet

### 2. CRUD TÃ¢ches
- CrÃ©er une nouvelle tÃ¢che dans un projet
- Lire/consulter les informations d'une tÃ¢che
- Modifier une tÃ¢che existante (tous les champs)
- Supprimer une tÃ¢che

### 3. RequÃªtes intelligentes
- Rechercher des projets/tÃ¢ches selon des critÃ¨res complexes en langage naturel
- Filtrer par dates, statuts, types, etc.
- Identifier les tÃ¢ches urgentes, en retard, Ã  venir

### 4. Suggestions
- Proposer des optimisations de planning
- SuggÃ©rer la priorisation de tÃ¢ches
- Aider Ã  la dÃ©composition de tÃ¢ches complexes
- Identifier les conflits d'Ã©chÃ©ances

## Architecture technique

### 1. DÃ©pendances
```
ollama>=0.1.0  # Client Python pour Ollama
```

### 2. Modules Ã  crÃ©er

#### `ai_config.py`
Configuration centralisÃ©e pour Ollama :
- URL du serveur Ollama (dÃ©faut: http://localhost:11434)
- ModÃ¨le Ã  utiliser (dÃ©faut: llama3.2)
- ParamÃ¨tres de gÃ©nÃ©ration (temperature, etc.)

#### `ai_assistant.py`
Module principal d'interaction avec l'IA :

**Fonctions principales** :
- `define_tools()` : DÃ©finit les outils disponibles pour l'IA
- `execute_tool_call(tool_name, arguments)` : ExÃ©cute un appel de fonction
- `process_user_request(prompt)` : Point d'entrÃ©e principal

**Tools disponibles** (mapping vers fonctions existantes) :
- `creer_projet(titre, description, statut)`
- `modifier_projet(projet_id, titre, description, statut)`
- `supprimer_projet(projet_id)`
- `lister_projets(statut)`
- `creer_tache(projet_id, type, titre, ...)`
- `modifier_tache(tache_id, ...)`
- `supprimer_tache(tache_id)`
- `lister_taches(projet_id, type)`
- `rechercher_projets_taches(criteres)` : Nouvelle fonction de recherche intelligente

### 3. IntÃ©gration dans l'interface

#### Modifications de `kanban_view_nice.py`

**Dans la fonction `send_prompt()`** :
```python
def send_prompt():
    user_prompt = prompt_input.value
    if user_prompt.strip():
        # Appeler l'IA
        result = ai_assistant.process_user_request(user_prompt)

        # Afficher le rÃ©sultat
        ai_response['text'] += f"\n\n> Vous: {user_prompt}\n\n{result['message']}\n"
        response_area.set_text(ai_response['text'])

        # RafraÃ®chir l'affichage si modification
        if result['modified']:
            refresh_kanban()

        prompt_input.value = ''
```

**Gestion des erreurs** :
- Messages clairs en cas d'erreur
- Suggestions de correction si commande mal formulÃ©e
- Confirmation pour les opÃ©rations de suppression

## Exemples d'utilisation

### CrÃ©ation de projets
```
"CrÃ©e un projet 'Stage entreprise' avec Ã©chÃ©ance au 15 juin"
"Ajoute un nouveau projet 'MÃ©moire de fin d'Ã©tudes'"
```

### Gestion de tÃ¢ches
```
"Ajoute une tÃ¢che 'RÃ©diger introduction' dans le projet MÃ©moire"
"Passe la tÃ¢che 'Recherche bibliographique' Ã  75% d'avancement"
"Liste toutes les tÃ¢ches urgentes (Ã©chÃ©ance < 48h)"
```

### Modifications
```
"Change le titre du projet 1 en 'Projet de recherche'"
"DÃ©place l'Ã©chÃ©ance de la tÃ¢che 5 au 20 mai"
"Marque le projet 'Stage' comme fermÃ©"
```

### Recherches intelligentes
```
"Montre-moi tous les projets ouverts"
"Quelles sont mes tÃ¢ches en retard ?"
"Liste les rÃ©unions de cette semaine"
"Quelles tÃ¢ches sont Ã  moins de 30% d'avancement ?"
```

### Suggestions
```
"Aide-moi Ã  organiser mes tÃ¢ches par prioritÃ©"
"DÃ©compose la tÃ¢che 'RÃ©diger le rapport' en sous-tÃ¢ches"
"Y a-t-il des conflits d'Ã©chÃ©ances dans mes projets ?"
```

## Plan d'implÃ©mentation

### Phase 1 : Configuration et setup
1. Installer Ollama sur le systÃ¨me
2. Ajouter la dÃ©pendance `ollama` Ã  requirements.txt
3. CrÃ©er `ai_config.py` avec la configuration
4. Documenter l'installation pour les Ã©tudiants

### Phase 2 : Module AI Assistant
1. CrÃ©er `ai_assistant.py`
2. DÃ©finir tous les tools avec leurs schÃ©mas JSON
3. ImplÃ©menter `define_tools()` : mapping vers projet_db/tache_db
4. ImplÃ©menter `execute_tool_call()` : exÃ©cution sÃ©curisÃ©e
5. ImplÃ©menter `process_user_request()` : orchestration complÃ¨te
6. Ajouter la fonction `rechercher_projets_taches()` pour recherches complexes

### Phase 3 : IntÃ©gration interface
1. Modifier `send_prompt()` dans kanban_view_nice.py
2. Ajouter la gestion des erreurs avec messages clairs
3. ImplÃ©menter le rafraÃ®chissement automatique aprÃ¨s modification
4. AmÃ©liorer l'affichage des rÃ©ponses (formatting, couleurs)

### Phase 4 : Tests et documentation
1. Tester toutes les opÃ©rations CRUD via prompts
2. Tester les recherches intelligentes
3. Valider les suggestions
4. CrÃ©er `AI_EXAMPLES.md` avec exemples de prompts
5. RÃ©diger documentation pour Ã©tudiants (README_IA.md)

### Phase 5 : Optimisations
1. Ajouter un historique des conversations
2. ImplÃ©menter des confirmations pour actions critiques
3. Ajouter des raccourcis/templates de prompts
4. Optimiser les performances

## Avantages pÃ©dagogiques

Cette approche est idÃ©ale pour des Ã©tudiants car :

1. **ComprÃ©hensible** : Pas de concepts complexes, juste du Python et du JSON
2. **Gratuite** : Pas de frais cachÃ©s, fonctionne complÃ¨tement hors ligne
3. **Modulaire** : Chaque composant est indÃ©pendant et testable
4. **RÃ©utilisable** : Le code existant est prÃ©servÃ© et enrichi
5. **Ã‰volutive** : Facile d'ajouter de nouveaux tools ou capacitÃ©s
6. **Pratique** : Montre concrÃ¨tement comment une IA interagit avec une base de donnÃ©es

## SÃ©curitÃ© et bonnes pratiques

- Validation des entrÃ©es avant exÃ©cution
- Confirmation requise pour suppressions
- Logs des actions effectuÃ©es par l'IA
- Gestion propre des erreurs de connexion DB
- Limitation des requÃªtes complexes pour Ã©viter surcharge

## Ressources pour les Ã©tudiants

### Documentation Ollama
- Site officiel : https://ollama.com
- Documentation API : https://github.com/ollama/ollama/blob/main/docs/api.md
- ModÃ¨les disponibles : https://ollama.com/library

### Tutoriels recommandÃ©s
- "Getting Started with Ollama and Python"
- "Function Calling with Local LLMs"
- "Building AI Assistants with Ollama"
